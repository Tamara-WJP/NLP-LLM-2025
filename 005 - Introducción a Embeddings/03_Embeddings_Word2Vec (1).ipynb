{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cuerno Clase PLN - Introducción a Word Embeddings y Word2Vec\n",
        "\n",
        "**Objetivos:**\n",
        "*   Entender las limitaciones de BoW/TF-IDF.\n",
        "*   Comprender el concepto de Word Embeddings (vectores densos semánticos).\n",
        "*   Conocer las arquitecturas Word2Vec (CBOW y Skip-gram).\n",
        "*   Aprender a cargar y usar vectores Word2Vec pre-entrenados con `gensim`.\n",
        "*   Explorar similitud y analogías entre palabras.\n",
        "*   (Opcional) Visualizar embeddings.\n",
        "\n",
        "**Agenda:**\n",
        "1.  Instalaciones e Importaciones\n",
        "2.  ¿Por qué necesitamos algo mejor que BoW/TF-IDF?\n",
        "3.  La Idea Clave: Word Embeddings\n",
        "4.  Word2Vec: Aprendiendo de los Vecinos (CBOW y Skip-gram)\n",
        "5.  Usando Vectores Pre-entrenados (¡Importante!)\n",
        "6.  Cargando Vectores con Gensim\n",
        "7.  Explorando los Vectores: Similitud y Analogías\n",
        "8.  (Opcional) Visualización de Embeddings\n",
        "9.  Micro-Laboratorio (Ejercicio Práctico)\n",
        "10. Brainstorming: Sesgos en Embeddings"
      ],
      "metadata": {
        "id": "hScKD4A12Eky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Instalaciones e Importaciones"
      ],
      "metadata": {
        "id": "NufE2aqOeofZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Idwdlzs42Aew"
      },
      "outputs": [],
      "source": [
        "# Gensim es la librería clave para trabajar con Word2Vec, GloVe, FastText\n",
        "!pip install gensim matplotlib seaborn scikit-learn > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gensim -y # Remove the existing gensim installation\n",
        "!pip install gensim # Reinstall gensim to align with the NumPy version\n",
        "# Restart the kernel to ensure the changes take effect"
      ],
      "metadata": {
        "id": "aRxOgRsAZ9g-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68bacf6-8de1-4c45-bd82-95ca271f8332"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.4.0\n",
            "Uninstalling gensim-4.4.0:\n",
            "  Successfully uninstalled gensim-4.4.0\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Using cached gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors # Para cargar modelos pre-entrenados\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Para evitar warnings de gensim a veces\n",
        "\n",
        "# Para visualización (Opcional)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "print(\"Librerías importadas.\")\n",
        "# Nota: Más adelante necesitaremos descargar un archivo de vectores pre-entrenados."
      ],
      "metadata": {
        "id": "NJC-y1t-2h1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654e1803-858e-4212-e3f0-4de8a3c04033"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Librerías importadas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ¿Por qué necesitamos algo mejor que BoW/TF-IDF?\n",
        "\n",
        "Recordemos rápido las limitaciones de las representaciones que vimos previamente:\n",
        "\n",
        "*   **Vectores Enormes y Dispersos:** Si tenemos 50,000 palabras únicas, cada documento es un vector de 50,000 dimensiones, ¡casi todo ceros! Muy ineficiente.\n",
        "*   **No Capturan Significado (Semántica):**\n",
        "    *   Las palabras \"coche\" y \"auto\" son sinónimos, pero para BoW/TF-IDF son tan diferentes entre sí como lo son de \"banana\". Cada una es una columna distinta.\n",
        "    *   No hay noción de que \"perro\" y \"gato\" son más parecidos entre sí que \"perro\" y \"mesa\".\n",
        "*   **Ignoran el Orden de las Palabras:** \"Perro muerde a hombre\" y \"Hombre muerde a perro\" tienen representaciones muy similares (mismas palabras, distintos conteos quizás, pero misma bolsa).\n",
        "\n",
        "Necesitamos una forma de representar palabras que capture su **significado** y las **relaciones** entre ellas."
      ],
      "metadata": {
        "id": "azgmHuDI5urj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. La Idea Clave: Word Embeddings\n",
        "\n",
        "¡Aquí entran los **Word Embeddings** (incrustaciones de palabras)!\n",
        "\n",
        "*   **Idea Central:** Representar cada palabra como un **vector denso** (la mayoría de valores NO son cero) en un espacio de **menor dimensión** (ej: 50, 100, 300 dimensiones, ¡no 50,000!).\n",
        "*   **La \"Magia\":** Estos vectores no son aleatorios. Se aprenden de grandes cantidades de texto de forma que palabras con **significados similares** terminan teniendo **vectores cercanos** en ese espacio vectorial.\n",
        "    *   El vector de \"gato\" estará cerca del vector de \"perro\".\n",
        "    *   El vector de \"contento\" estará cerca del vector de \"feliz\".\n",
        "*   **Analogía:** Imaginen un mapa. Cada ciudad es una palabra. Las ciudades cercanas en el mapa representan palabras semánticamente relacionadas.\n",
        "*   **¡Álgebra de Palabras!** Lo más sorprendente es que la estructura de este espacio vectorial captura relaciones semánticas y sintácticas de forma lineal. El ejemplo clásico:\n",
        "    *   `vector('Rey') - vector('Hombre') + vector('Mujer') ≈ vector('Reina')`\n",
        "    *   Podemos \"operar\" con los significados de las palabras usando sus vectores."
      ],
      "metadata": {
        "id": "aX00cMqM6DaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Word2Vec: Aprendiendo de los Vecinos (CBOW y Skip-gram)\n",
        "\n",
        "Word2Vec (Mikolov et al., Google, 2013) fue uno de los métodos pioneros y más populares para aprender estos embeddings.\n",
        "\n",
        "*   **Principio Fundamental (Hipótesis Distribucional):** *Una palabra se caracteriza por las compañías que mantiene*. Es decir, aprendemos el significado de una palabra observando las palabras que aparecen a su alrededor (su contexto).\n",
        "*   **Arquitecturas (Redes Neuronales Superficiales):**\n",
        "    *   **CBOW (Continuous Bag-of-Words):** Dado un conjunto de palabras de contexto (ej: las 2 palabras antes y las 2 después), intenta **predecir la palabra central**. Es más rápido de entrenar y bueno para palabras frecuentes.\n",
        "        *   `[la, casa, ___, bonita] -> predecir 'es'`\n",
        "    *   **Skip-gram:** Dada la palabra central, intenta **predecir las palabras de su contexto**. Es más lento pero generalmente funciona mejor con palabras infrecuentes y captura mejor relaciones semánticas más finas. Suele ser el preferido si el rendimiento es clave.\n",
        "        *   `'es' -> predecir [la, casa, ___, bonita]`\n",
        "*   **¿Cómo se aprende?:** La red neuronal (muy simple, usualmente una sola capa oculta) ajusta los vectores (embeddings) de entrada y salida para mejorar en la tarea de predicción (sea CBOW o Skip-gram). Los detalles técnicos (Negative Sampling, Hierarchical Softmax) son optimizaciones para hacerlo eficiente, pero la idea es que los vectores se ajustan para que palabras que aparecen en contextos similares terminen con vectores similares.\n",
        "*   **Resultado:** Una \"tabla de consulta\" (o diccionario) donde cada palabra del vocabulario tiene asociado un vector denso aprendido (el embedding)."
      ],
      "metadata": {
        "id": "udidG2ZG6WFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Usando Vectores Pre-entrenados (¡Importante!)\n",
        "\n",
        "Entrenar modelos Word2Vec (o GloVe, FastText) desde cero requiere:\n",
        "1.  **Muchísimo texto:** Millones o miles de millones de palabras (ej: toda la Wikipedia, noticias de años).\n",
        "2.  **Mucho tiempo computacional:** Horas o días, incluso con hardware potente.\n",
        "\n",
        "**¡La buena noticia!** Investigadores y empresas ya han hecho este trabajo pesado y publican los **modelos pre-entrenados**. Nosotros podemos simplemente descargarlos y usarlos.\n",
        "\n",
        "Para español, una fuente común y de buena calidad son los vectores entrenados sobre el **Spanish Billion Words Corpus (SBWC)** por la Universidad de Chile. También hay otros, como los de FastText.\n",
        "\n",
        "**Tarea:** Necesitamos descargar uno de estos archivos. Busquen en la web \"Spanish Word Embeddings SBWC download\" o similar.\n",
        "*   Recomendado: Descargar el archivo `.bin` (formato binario, más compacto) si está disponible. Si no, el `.vec` (formato texto). Suelen ser archivos grandes (varios GB!).\n",
        "*   **Importante:** Una vez descargado, anoten la ruta completa donde guardaron el archivo en su máquina o Google Drive.\n",
        "\n",
        "[Spanish Word Embeddings SBWC download](https://github.com/dccuchile/spanish-word-embeddings?tab=readme-ov-file) [Mirror](https://drive.google.com/file/d/1rSI0q8J_USo1GpPfzUOa7pNgmLc66svv/edit)"
      ],
      "metadata": {
        "id": "LP0X-ilR6WCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Cargando Vectores con Gensim\n",
        "\n",
        "Una vez que tenemos el archivo (`.bin` o `.vec`), usamos `gensim` para cargarlo.\n",
        "\n",
        "**¡Reemplacen `'RUTA_AL_ARCHIVO_DESCARGADO.bin'` con la ruta real donde guardaron el archivo!**\n",
        "\n",
        "Si están en Colab, pueden subir el archivo a su sesión (tardará si es grande) o montarlo desde Google Drive."
      ],
      "metadata": {
        "id": "cooGKzHS6V-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "Lb3kROviDFUX",
        "outputId": "53a1eb14-0dac-42f4-c025-197264e91920"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ¡¡¡ MODIFICAR ESTA RUTA !!! ---\n",
        "# Ejemplo de ruta (si lo subieron a Colab o está en el mismo directorio):\n",
        "# path_to_vectors = 'SBW-vectors-300-min5.bin.gz' # Reemplazar con el nombre exacto\n",
        "# Ejemplo de ruta (si está en Google Drive montado en /content/drive/MyDrive/):\n",
        "# path_to_vectors = '/content/drive/MyDrive/Colab Notebooks/embeddings/SBW-vectors-300-min5.bin.gz' # Ajustar según su estructura de Drive\n",
        "path_to_vectors = '/content/drive/MyDrive/Clases/HABLA/004/PRA/vectors/SBW-vectors-300-min5.bin.gz' # Reemplazar con el nombre exacto"
      ],
      "metadata": {
        "id": "up9W-fYrKWWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intentar cargar el modelo\n",
        "try:\n",
        "    # Si es formato binario (.bin, .bin.gz): binary=True\n",
        "    # Si es formato texto (.vec, .txt, .vec.gz): binary=False\n",
        "    print(\"Cargando vectores... (puede tardar unos minutos)\")\n",
        "    word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors, binary=True)\n",
        "    # Si da error con binary=True, y tu archivo es .vec o .txt, prueba con binary=False\n",
        "    # word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors, binary=False)\n",
        "    print(f\"¡Vectores cargados! Vocabulario: {len(word_vectors.index_to_key)} palabras. Dimensión: {word_vectors.vector_size}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: No se encontró el archivo en la ruta '{path_to_vectors}'.\")\n",
        "    print(\"Por favor, descarga los vectores pre-entrenados (ej: SBWC)\")\n",
        "    print(\"y asegúrate de que la variable 'path_to_vectors' tenga la ruta correcta.\")\n",
        "    word_vectors = None # Dejarlo como None si no se pudo cargar"
      ],
      "metadata": {
        "id": "AMvrBByh53cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Explorando los Vectores: Similitud y Analogías\n",
        "\n",
        "¡Ahora viene lo divertido! Si los vectores se cargaron (`word_vectors` no es `None`), podemos explorarlos."
      ],
      "metadata": {
        "id": "P8qDLgY7Q9JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificación de existencia en el vocabulario"
      ],
      "metadata": {
        "id": "xvnGdmrkSBXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'word_vectors' in locals() and word_vectors:\n",
        "    print(\"Explorando las propiedades de los vectores de palabras...\")\n",
        "\n",
        "    # Verificar si una palabra está en el vocabulario\n",
        "    print(f\"\\n'rey' in vocabulario? {'rey' in word_vectors}\")\n",
        "    print(f\"'programacion' in vocabulario? {'programacion' in word_vectors}\") # Ojo: puede estar 'programación'\n",
        "    print(f\"'programación' in vocabulario? {'programación' in word_vectors}\")\n",
        "    print(f\"'casa' in vocabulario? {'casa' in word_vectors}\")\n",
        "    print(f\"'qwerty' in vocabulario? {'qwerty' in word_vectors}\") # Probablemente no\n",
        "else:\n",
        "    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Por favor, carga el modelo de vectores de palabras primero.\")"
      ],
      "metadata": {
        "id": "R9Tgwu4YQ90_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtener vector de una palabra"
      ],
      "metadata": {
        "id": "Ig16VqvDSCaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'word_vectors' in locals() and word_vectors:\n",
        "    # Obtener el vector de una palabra\n",
        "    if 'rey' in word_vectors:\n",
        "        vector_rey = word_vectors['rey']\n",
        "        print(f\"\\nDimensiones del vector 'rey': {vector_rey.shape}\")\n",
        "        # print(\"Primeros 10 valores:\", vector_rey[:10]) # Descomentar para ver un trozo\n",
        "    else:\n",
        "         print(\"\\nLa palabra 'rey' no se encuentra en el vocabulario.\")\n",
        "else:\n",
        "    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"
      ],
      "metadata": {
        "id": "cs4ozCOkR7mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Encontrar palabras similares"
      ],
      "metadata": {
        "id": "sFQXrNyPSGn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'word_vectors' in locals() and word_vectors:\n",
        "    print(\"\\n--- Palabras más similares ---\")\n",
        "    try:\n",
        "        similares_rey = word_vectors.most_similar('rey', topn=5)\n",
        "        print(\"Más similares a 'rey':\", similares_rey)\n",
        "    except KeyError:\n",
        "        print(\"Palabra 'rey' no encontrada.\")\n",
        "\n",
        "    try:\n",
        "        similares_perro = word_vectors.most_similar('perro', topn=5)\n",
        "        print(\"Más similares a 'perro':\", similares_perro)\n",
        "    except KeyError:\n",
        "        print(\"Palabra 'perro' no encontrada.\")\n",
        "else:\n",
        "    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"
      ],
      "metadata": {
        "id": "Isgivtu-b6vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'word_vectors' in locals() and word_vectors:\n",
        "    print(\"\\n--- Palabras más similares ---\")\n",
        "    try:\n",
        "        # Probar con una palabra que podría no estar exacta: 'computadora' vs 'computador'\n",
        "        palabra_compu = 'computadora' if 'computadora' in word_vectors else ('computador' if 'computador' in word_vectors else None)\n",
        "        if palabra_compu:\n",
        "            similares_compu = word_vectors.most_similar(palabra_compu, topn=5)\n",
        "            print(f\"Más similares a '{palabra_compu}':\", similares_compu)\n",
        "        else:\n",
        "            print(\"No se encontró 'computadora' ni 'computador'.\")\n",
        "    except KeyError:\n",
        "        print(\"Error buscando similares a computadora/or.\")"
      ],
      "metadata": {
        "id": "r6KKEre6cGPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizar analogías (Álgebra de palabras)"
      ],
      "metadata": {
        "id": "eT7FmwLESMgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'word_vectors' in locals() and word_vectors:\n",
        "    print(\"\\n--- Analogías ---\")\n",
        "    # Analogía clásica: rey - hombre + mujer = ? (esperamos reina)\n",
        "    try:\n",
        "        # Verificar si las palabras necesarias existen antes de la analogía\n",
        "        if all(p in word_vectors for p in ['rey', 'hombre', 'mujer']):\n",
        "            analogia_reina = word_vectors.most_similar(positive=['rey', 'mujer'], negative=['hombre'], topn=1)\n",
        "            print(\"rey - hombre + mujer ≈\", analogia_reina)\n",
        "        else:\n",
        "             missing_words = [p for p in ['rey', 'hombre', 'mujer'] if p not in word_vectors]\n",
        "             print(f\"Faltan palabras para la analogía rey/reina: {missing_words}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error en analogía rey/reina: falta la palabra '{e.args[0]}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error inesperado en la analogía rey/reina: {e}\")\n",
        "\n",
        "\n",
        "    # Analogía de capitales: España - Madrid + París = ? (esperamos Francia)\n",
        "    try:\n",
        "        # Ojo: verificar que las palabras estén! Usar .lower() si es necesario\n",
        "        palabras_capitales = ['españa', 'madrid', 'parís'] # Usar minúsculas si el modelo es cased\n",
        "        if all(p in word_vectors for p in palabras_capitales):\n",
        "            analogia_francia = word_vectors.most_similar(positive=['españa', 'parís'], negative=['madrid'], topn=1)\n",
        "            print(\"España - Madrid + París ≈\", analogia_francia)\n",
        "        else:\n",
        "            missing_words = [p for p in palabras_capitales if p not in word_vectors]\n",
        "            print(f\"Faltan palabras para la analogía de capitales: {missing_words}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error en analogía capitales: falta la palabra '{e.args[0]}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error inesperado en la analogía capitales: {e}\")\n",
        "\n",
        "\n",
        "    # Analogía de verbos: correr - corriendo + comiendo = ? (esperamos comer)\n",
        "    try:\n",
        "        palabras_verbos = ['correr', 'corriendo', 'comiendo']\n",
        "        if all(p in word_vectors for p in palabras_verbos):\n",
        "           analogia_comer = word_vectors.most_similar(positive=['correr', 'comiendo'], negative=['corriendo'], topn=1)\n",
        "           print(\"correr - corriendo + comiendo ≈\", analogia_comer)\n",
        "        else:\n",
        "            missing_words = [p for p in palabras_verbos if p not in word_vectors]\n",
        "            print(f\"Faltan palabras para la analogía de verbos: {missing_words}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error en analogía verbos: falta la palabra '{e.args[0]}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error inesperado en la analogía verbos: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"
      ],
      "metadata": {
        "id": "LJltoUTsR7hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcular similitud coseno"
      ],
      "metadata": {
        "id": "YWNV-7GLSRuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'word_vectors' in locals() and word_vectors:\n",
        "    print(\"\\n--- Similitud Coseno ---\")\n",
        "    try:\n",
        "        if 'perro' in word_vectors and 'gato' in word_vectors:\n",
        "            sim_perro_gato = word_vectors.similarity('perro', 'gato')\n",
        "            print(f\"Similitud('perro', 'gato'): {sim_perro_gato:.4f}\")\n",
        "        else:\n",
        "            print(\"Faltan 'perro' o 'gato' para calcular similitud.\")\n",
        "    except KeyError:\n",
        "        print(\"Error al calcular similitud entre 'perro' y 'gato' (KeyError).\") # Manejo redundante si se verifica antes\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error inesperado calculando similitud perro/gato: {e}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        if 'perro' in word_vectors and 'mesa' in word_vectors:\n",
        "            sim_perro_mesa = word_vectors.similarity('perro', 'mesa')\n",
        "            print(f\"Similitud('perro', 'mesa'): {sim_perro_mesa:.4f}\")\n",
        "        else:\n",
        "            print(\"Faltan 'perro' o 'mesa' para calcular similitud.\")\n",
        "    except KeyError:\n",
        "        print(\"Error al calcular similitud entre 'perro' y 'mesa' (KeyError).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error inesperado calculando similitud perro/mesa: {e}\")\n",
        "\n",
        "    try:\n",
        "        if 'feliz' in word_vectors and 'contento' in word_vectors:\n",
        "            sim_feliz_contento = word_vectors.similarity('feliz', 'contento')\n",
        "            print(f\"Similitud('feliz', 'contento'): {sim_feliz_contento:.4f}\")\n",
        "        else:\n",
        "            print(\"Faltan 'feliz' o 'contento' para calcular similitud.\")\n",
        "    except KeyError:\n",
        "        print(\"Error al calcular similitud entre 'feliz' y 'contento' (KeyError).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error inesperado calculando similitud feliz/contento: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nLa variable 'word_vectors' no está definida o está vacía. Esta celda requiere que el modelo esté cargado.\")"
      ],
      "metadata": {
        "id": "4NL1TC3WSRa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. (Opcional) Visualización de Embeddings\n",
        "\n",
        "Como los vectores tienen muchas dimensiones (ej: 300), no podemos graficarlos directamente. Pero podemos usar técnicas de **reducción de dimensionalidad** como PCA o (mejor para visualización) t-SNE para proyectarlos a 2D y ver si las palabras relacionadas se agrupan."
      ],
      "metadata": {
        "id": "Gv35TEoHSmzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualización Opcional ---\n",
        "if word_vectors:\n",
        "    # Lista de palabras para visualizar (elegir grupos relacionados)\n",
        "    palabras_a_visualizar = [\n",
        "        'perro', 'gato', 'caballo', 'pez', 'pájaro', # Animales\n",
        "        'casa', 'apartamento', 'hogar', 'edificio', # Hogar\n",
        "        'comer', 'beber', 'cocinar', 'correr', 'saltar', # Verbos\n",
        "        'feliz', 'contento', 'triste', 'enojado', # Emociones\n",
        "        'rey', 'reina', 'hombre', 'mujer' # Relación clásica\n",
        "    ]\n",
        "\n",
        "    # Filtrar palabras que SÍ están en el vocabulario\n",
        "    palabras_presentes = [p for p in palabras_a_visualizar if p in word_vectors]\n",
        "    vectores_presentes = [word_vectors[p] for p in palabras_presentes]\n",
        "\n",
        "    if len(vectores_presentes) > 1: # Necesitamos al menos 2 puntos para visualizar\n",
        "        print(f\"\\nVisualizando {len(palabras_presentes)} palabras...\")\n",
        "\n",
        "        # Reducción de dimensionalidad con t-SNE (suele dar mejores clusters visuales)\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(palabras_presentes)-1)) # Perplexity debe ser menor que n_samples\n",
        "        vectores_2d = tsne.fit_transform(np.array(vectores_presentes))\n",
        "\n",
        "        # Crear el gráfico\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.scatterplot(x=vectores_2d[:, 0], y=vectores_2d[:, 1])\n",
        "\n",
        "        # Añadir etiquetas a los puntos\n",
        "        for i, palabra in enumerate(palabras_presentes):\n",
        "            plt.annotate(palabra, (vectores_2d[i, 0], vectores_2d[i, 1]), fontsize=9)\n",
        "\n",
        "        plt.title('Visualización de Word Embeddings (t-SNE)')\n",
        "        plt.xlabel('Dimensión 1')\n",
        "        plt.ylabel('Dimensión 2')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No hay suficientes palabras presentes en el vocabulario para visualizar.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron cargar los vectores. Saltando la visualización.\")"
      ],
      "metadata": {
        "id": "gTyb2KT7R7dw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "0f285f0d-dede-424e-b5c3-2a453b66c91e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'word_vectors' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-176905878.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Visualización Opcional ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Lista de palabras para visualizar (elegir grupos relacionados)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     palabras_a_visualizar = [\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'perro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gato'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caballo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pez'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pájaro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Animales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_vectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Micro-Laboratorio (Ejercicio Práctico)\n",
        "\n",
        "**Consigna:** (Asumiendo que `word_vectors` se cargó correctamente)\n",
        "\n",
        "1.  **Exploración de Similitud:**\n",
        "    *   Elegir 5 palabras que les interesen (intenten variar: un lugar, una profesión, un concepto abstracto, una comida, un sentimiento).\n",
        "    *   Para cada una, usar `word_vectors.most_similar()` para encontrar las 5 palabras más parecidas.\n",
        "    *   Anotar los resultados. ¿Les parecen lógicos? ¿Hay alguna similitud sorprendente o extraña?\n",
        "\n",
        "2.  **Prueba de Analogías:**\n",
        "    *   Inventar y probar 3 analogías diferentes usando `word_vectors.most_similar(positive=[...], negative=[...])`.\n",
        "    *   Ideas:\n",
        "        *   `programador` es a `computadora` como `médico` es a `?`\n",
        "        *   `Argentina` es a `peso` como `Japón` es a `?`\n",
        "        *   `caminar` es a `pierna` como `hablar` es a `?`\n",
        "    *   Verificar que todas las palabras de la analogía estén en el vocabulario antes de probarla.\n",
        "    *   Anotar los resultados. ¿Funcionan las analogías como esperaban?\n",
        "\n",
        "3.  **(Opcional) Medir Similitud:**\n",
        "    *   Elegir 3 pares de palabras:\n",
        "        *   Un par de sinónimos claros (ej: `estudiante`, `alumno`).\n",
        "        *   Un par de antónimos (ej: `grande`, `pequeño`).\n",
        "        *   Un par de palabras no relacionadas (ej: `nube`, `zapato`).\n",
        "    *   Calcular `word_vectors.similarity()` para cada par.\n",
        "    *   ¿Los valores de similitud reflejan la relación entre las palabras? (Esperamos alta para sinónimos, baja/media-baja para no relacionadas, y ¿qué pasa con antónimos?).\n",
        "\n",
        "**¡Asegúrense de que las palabras que usan existan en `word_vectors`!**"
      ],
      "metadata": {
        "id": "nC63WVgQSvzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def in_vocab(wv, word):\n",
        "    \"\"\"Devuelve True si la palabra está en el vocabulario (maneja strings).\"\"\"\n",
        "    try:\n",
        "        return word in wv.key_to_index\n",
        "    except Exception:\n",
        "        # compatibilidad con versiones antiguas\n",
        "        return word in wv.vocab\n",
        "\n",
        "def safe_most_similar(wv, word, topn=5):\n",
        "    \"\"\"Devuelve topn most_similar si la palabra existe, sino None.\"\"\"\n",
        "    if not in_vocab(wv, word):\n",
        "        return None\n",
        "    try:\n",
        "        return wv.most_similar(positive=[word], topn=topn)\n",
        "    except Exception as e:\n",
        "        # fallback por si la API varía\n",
        "        vec = wv[word]\n",
        "        sims = []\n",
        "        for other in list(wv.key_to_index.keys())[:200000]:\n",
        "            if other == word:\n",
        "                continue\n",
        "            sims.append((other, float(np.dot(vec, wv[other]) / (np.linalg.norm(vec)*np.linalg.norm(wv[other])))))\n",
        "        sims = sorted(sims, key=lambda x: x[1], reverse=True)[:topn]\n",
        "        return sims\n",
        "\n",
        "def safe_similarity(wv, w1, w2):\n",
        "    \"\"\"Calcula similitud (coseno) entre dos palabras, con manejo de errores.\"\"\"\n",
        "    if not in_vocab(wv, w1) or not in_vocab(wv, w2):\n",
        "        return None\n",
        "    try:\n",
        "        return float(wv.similarity(w1, w2))\n",
        "    except Exception:\n",
        "        v1, v2 = wv[w1], wv[w2]\n",
        "        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
        "\n",
        "def safe_analogy(wv, positive=None, negative=None, topn=5):\n",
        "    \"\"\"Resuelve analogía positive - negative, previa comprobación de vocabulario.\n",
        "       positive/negative son listas de palabras.\"\"\"\n",
        "    positive = positive or []\n",
        "    negative = negative or []\n",
        "    # comprobar vocab\n",
        "    for w in (positive + negative):\n",
        "        if not in_vocab(wv, w):\n",
        "            return {'error': f\"La palabra '{w}' no está en el vocabulario.\"}\n",
        "    try:\n",
        "        res = wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
        "        return {'result': res}\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "# ------------------------------\n",
        "# Comprueba que word_vectors existe\n",
        "# ------------------------------\n",
        "try:\n",
        "    word_vectors  # noqa: F821\n",
        "except NameError:\n",
        "    raise RuntimeError(\"La variable `word_vectors` no está definida en el entorno. Carga un KeyedVectors antes de ejecutar este bloque.\")\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Exploración de similitud: elegir 5 palabras\n",
        "# ------------------------------\n",
        "palabras = [\"programador\", \"amor\", \"pizza\", \"Argentina\", \"feliz\"]  # puedes cambiar por las que prefieras\n",
        "\n",
        "exploracion = []\n",
        "for p in palabras:\n",
        "    if in_vocab(word_vectors, p):\n",
        "        similares = safe_most_similar(word_vectors, p, topn=5)\n",
        "        exploracion.append({\n",
        "            \"palabra\": p,\n",
        "            \"en_vocab\": True,\n",
        "            \"top5_similares\": similares\n",
        "        })\n",
        "    else:\n",
        "        exploracion.append({\n",
        "            \"palabra\": p,\n",
        "            \"en_vocab\": False,\n",
        "            \"top5_similares\": None\n",
        "        })\n",
        "\n",
        "# Imprimir resultados legibles\n",
        "print(\"=== EXPLORACIÓN DE SIMILITUD ===\\n\")\n",
        "for item in exploracion:\n",
        "    if not item[\"en_vocab\"]:\n",
        "        print(f\"- '{item['palabra']}' NO está en el vocabulario. Saltando.\\n\")\n",
        "        continue\n",
        "    print(f\"- '{item['palabra']}' -> 5 palabras más similares:\")\n",
        "    for w, score in item['top5_similares']:\n",
        "        print(f\"    {w:20}  sim={score:.4f}\")\n",
        "    print()\n",
        "\n",
        "# También lo dejamos como DataFrame (útil para exportar/copiar)\n",
        "df_expl = pd.DataFrame([\n",
        "    {\"palabra\": it[\"palabra\"],\n",
        "     \"en_vocab\": it[\"en_vocab\"],\n",
        "     \"top5\": \", \".join([f\"{w} ({s:.3f})\" for w,s in it[\"top5_similares\"]]) if it[\"top5_similares\"] else \"\"}\n",
        "    for it in exploracion\n",
        "])\n",
        "display(df_expl)\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Prueba de Analogías\n",
        "# ------------------------------\n",
        "analogias = [\n",
        "    ([\"programador\"], [\"computadora\"], [\"medico\"]),   # queremos: programador:computadora :: medico:?\n",
        "    ([\"Argentina\"], [\"peso\"], [\"Japón\"]),             # Argentina:peso :: Japón:?\n",
        "    ([\"caminar\"], [\"pierna\"], [\"hablar\"])             # caminar:pierna :: hablar:?\n",
        "]\n",
        "\n",
        "# Nota: la forma usada en gensim es most_similar(positive=[A, C], negative=[B])\n",
        "# Para \"A es a B como C es a ?\" usamos positive=[C, B], negative=[A]\n",
        "# Pero aquí usaremos la forma: result = most_similar(positive=[C, B], negative=[A])\n",
        "\n",
        "print(\"=== ANALOGÍAS ===\\n\")\n",
        "analogy_results = []\n",
        "for A_pos, A_neg, C in analogias:\n",
        "    # re-arreglamos variables para la explicación humana\n",
        "    # En la tupla guardamos (A, B, C) como texto: A_pos[0] es A, A_neg[0] es B, C[0] es C en nuestro esquema\n",
        "    A = A_pos[0] if len(A_pos) else None\n",
        "    B = A_neg[0] if len(A_neg) else None\n",
        "    Cw = C[0] if len(C) else None\n",
        "\n",
        "    # Verificar vocabulario\n",
        "    missing = [w for w in (A, B, Cw) if w and not in_vocab(word_vectors, w)]\n",
        "    if missing:\n",
        "        msg = f\"No se puede resolver {A}:{B} :: {Cw}:? porque faltan en vocab: {missing}\"\n",
        "        print(msg)\n",
        "        analogy_results.append({\"A\":A, \"B\":B, \"C\":Cw, \"error\": msg})\n",
        "        continue\n",
        "\n",
        "    # construir positive/negative según gensim\n",
        "    positive = [Cw, B]\n",
        "    negative = [A]\n",
        "    res = safe_analogy(word_vectors, positive=positive, negative=negative, topn=5)\n",
        "    if 'error' in res:\n",
        "        print(f\"- Analogía {A}:{B} :: {Cw}:? -> ERROR: {res['error']}\")\n",
        "        analogy_results.append({\"A\":A, \"B\":B, \"C\":Cw, \"error\": res['error']})\n",
        "    else:\n",
        "        print(f\"- Analogía {A}:{B} :: {Cw}:? -> Top resultados:\")\n",
        "        for rword, score in res['result']:\n",
        "            print(f\"    {rword:20}  sim={score:.4f}\")\n",
        "        analogy_results.append({\"A\":A, \"B\":B, \"C\":Cw, \"result\": res['result']})\n",
        "    print()\n",
        "\n",
        "# Mostrar tabla resumen\n",
        "rows = []\n",
        "for ar in analogy_results:\n",
        "    if \"error\" in ar:\n",
        "        rows.append({\"A\":ar[\"A\"], \"B\":ar[\"B\"], \"C\":ar[\"C\"], \"top_result\": \"\", \"nota\": ar[\"error\"]})\n",
        "    else:\n",
        "        top = \", \".join([f\"{w}({s:.3f})\" for w,s in ar[\"result\"]])\n",
        "        rows.append({\"A\":ar[\"A\"], \"B\":ar[\"B\"], \"C\":ar[\"C\"], \"top_result\": top, \"nota\": \"\"})\n",
        "df_analog = pd.DataFrame(rows)\n",
        "display(df_analog)\n",
        "\n",
        "# ------------------------------\n",
        "# 3) (Opcional) Medir Similitud para 3 pares\n",
        "# ------------------------------\n",
        "pares = [\n",
        "    (\"estudiante\", \"alumno\"),   # sinónimos claros (esperado: alta similitud)\n",
        "    (\"grande\", \"pequeño\"),     # antónimos (pueden tener similitud alta en embeddings por contexto)\n",
        "    (\"nube\", \"zapato\")         # no relacionadas (baja similitud)\n",
        "]\n",
        "\n",
        "print(\"=== MEDICIÓN DE SIMILITUD (pares) ===\\n\")\n",
        "sim_rows = []\n",
        "for w1, w2 in pares:\n",
        "    sim = safe_similarity(word_vectors, w1, w2)\n",
        "    if sim is None:\n",
        "        nota = f\"Alguna palabra no está en vocabulario: {w1 if not in_vocab(word_vectors, w1) else ''} {w2 if not in_vocab(word_vectors, w2) else ''}\"\n",
        "        print(f\"- {w1}  /  {w2} -> NO DISPONIBLE. {nota}\")\n",
        "        sim_rows.append({\"w1\": w1, \"w2\": w2, \"similaridad\": None, \"nota\": nota})\n",
        "    else:\n",
        "        print(f\"- {w1:12} vs {w2:12} -> sim = {sim:.4f}\")\n",
        "        sim_rows.append({\"w1\": w1, \"w2\": w2, \"similaridad\": sim, \"nota\": \"\"})\n",
        "\n",
        "df_sims = pd.DataFrame(sim_rows)\n",
        "display(df_sims)"
      ],
      "metadata": {
        "id": "-ktJ7kTkR7XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Brainstorming: Sesgos en Embeddings\n",
        "\n",
        "Hemos visto que los embeddings capturan relaciones del lenguaje tal como aparecen en los datos de entrenamiento (el corpus masivo).\n",
        "\n",
        "**Pregunta clave:** Si esos datos contienen **sesgos sociales** (de género, raciales, de profesión, etc.), ¿qué creen que pasará con los embeddings?\n",
        "\n",
        "*   ¿Se reflejarán esos sesgos en las relaciones entre vectores? (Pista: ¡Sí!)\n",
        "    *   Ejemplo famoso: `doctor - hombre + mujer = ?` a veces da `enfermera` en modelos entrenados en textos antiguos o sesgados. `programador - hombre + mujer = ?` podía dar `ama de casa`.\n",
        "*   ¿Qué implicancias tiene esto si usamos estos embeddings para tareas como selección de personal, análisis de opiniones, o generación de texto?\n",
        "*   Si el corpus no representa bien la diversidad lingüística (dialectos, jergas, lenguaje inclusivo), ¿cómo afectará eso a los embeddings de esas palabras (si es que existen)?\n",
        "*   **¿Cómo podemos entrenar word embeddings que sean más inclusivos y representativos?** ¿Es posible \"limpiar\" o \"corregir\" (debias) los embeddings pre-entrenados? ¿Es nuestra responsabilidad como desarrolladores ser conscientes de esto y mitigarlo?\n",
        "\n",
        "**(Discusión en grupo)**"
      ],
      "metadata": {
        "id": "VtyameEOTEXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GUÍA DE ESTUDIO - WORD EMBEDDINGS Y WORD2VEC\n",
        "\n",
        "## Preguntas y Respuestas Clave\n",
        "\n",
        "### **Revolución Conceptual**\n",
        "\n",
        "**P: ¿Cuál es la diferencia fundamental entre BoW/TF-IDF y Word Embeddings?**  \n",
        "R: BoW/TF-IDF crea vectores dispersos de alta dimensión (una por palabra del vocabulario). Word Embeddings crea vectores densos de baja dimensión (ej: 300) donde palabras similares están cerca geométricamente.\n",
        "\n",
        "**P: ¿Qué significa que los embeddings capturen \"semántica\"?**  \n",
        "R: Que palabras con significados similares tienen vectores similares. \"auto\" y \"coche\" estarán cerca en el espacio vectorial, no en dimensiones separadas.\n",
        "\n",
        "**P: ¿Qué es la \"álgebra de palabras\" en embeddings?**  \n",
        "R: La capacidad de hacer operaciones matemáticas que preservan relaciones semánticas: vector('rey') - vector('hombre') + vector('mujer') ≈ vector('reina').\n",
        "\n",
        "### **Arquitecturas Word2Vec**\n",
        "\n",
        "**P: ¿Cuál es la diferencia entre CBOW y Skip-gram?**  \n",
        "R: CBOW predice palabra central dado contexto (rápido, bueno para palabras frecuentes). Skip-gram predice contexto dada palabra central (lento, mejor para palabras raras).\n",
        "\n",
        "**P: ¿Qué es la \"hipótesis distribucional\"?**  \n",
        "R: \"Una palabra se conoce por las compañías que mantiene\". Palabras que aparecen en contextos similares tienen significados similares.\n",
        "\n",
        "### **Implementación Práctica**\n",
        "\n",
        "**P: ¿Por qué usamos vectores pre-entrenados en lugar de entrenar desde cero?**  \n",
        "R: Entrenar requiere corpus masivos (millones de palabras) y días de computación. Los modelos pre-entrenados ya capturan patrones del idioma.\n",
        "\n",
        "**P: ¿Qué hace gensim.KeyedVectors?**  \n",
        "R: Permite cargar y manipular vectores pre-entrenados sin necesidad de re-entrenar. Incluye funciones para similitud y analogías.\n",
        "\n",
        "**P: ¿Cómo verificar si una palabra está en el vocabulario?**  \n",
        "R: `palabra in word_vectors` antes de usar most_similar() o similarity() para evitar errores.\n",
        "\n",
        "### **Análisis Semántico**\n",
        "\n",
        "**P: ¿Qué indica un valor de similitud coseno alto entre dos palabras?**  \n",
        "R: Que sus vectores apuntan en direcciones similares, sugiriendo relación semántica. Valores entre 0.3-0.7 son típicamente significativos.\n",
        "\n",
        "**P: ¿Por qué algunas analogías funcionan mejor que otras?**  \n",
        "R: Depende de qué tan consistentemente aparezcan esas relaciones en el corpus de entrenamiento. Relaciones muy frecuentes se capturan mejor.\n",
        "\n",
        "### **Limitaciones Importantes**\n",
        "\n",
        "**P: ¿Qué es el problema OOV (Out-of-Vocabulary)?**  \n",
        "R: Palabras no vistas durante entrenamiento no tienen vector. Word2Vec clásico no puede manejar palabras nuevas, errores tipográficos, o neologismos.\n",
        "\n",
        "**P: ¿Cómo reflejan sesgos los embeddings?**  \n",
        "R: Si el corpus contiene sesgos sociales, los vectores los perpetúan. \"doctor - hombre + mujer\" podría dar \"enfermera\" en modelos sesgados.\n",
        "\n",
        "## Puntos Clave para Recordar\n",
        "\n",
        "1. **Word2Vec captura semántica** que BoW/TF-IDF ignora completamente\n",
        "2. **Vectores densos de 300D > vectores dispersos de 50,000D**\n",
        "3. **Similitud geométrica = similitud semántica**\n",
        "4. **Analogías revelan estructura del lenguaje capturada**\n",
        "5. **Siempre verificar vocabulario** antes de usar palabras\n",
        "6. **Modelos pre-entrenados son la norma** para uso práctico\n",
        "\n",
        "## Errores Comunes a Evitar\n",
        "\n",
        "- Intentar usar palabras no presentes en vocabulario sin verificar\n",
        "- Esperar que analogías muy específicas siempre funcionen\n",
        "- Ignorar sesgos inherentes en modelos pre-entrenados\n",
        "- Comparar palabras muy diferentes y esperar similitud alta\n",
        "- No considerar que el modelo refleja el corpus de entrenamiento\n",
        "\n",
        "## Conexión con Próxima Clase\n",
        "\n",
        "Word2Vec tiene limitaciones: problema OOV y vocabulario fijo. La próxima clase explorará **FastText** y **GloVe**: arquitecturas que resuelven estos problemas y manejan palabras desconocidas.\n",
        "\n",
        "---\n",
        "*Consejo: Experimenta con analogías culturales argentinas: \"Messi - fútbol + música = ?\". ¿El modelo captura relaciones culturales locales?*"
      ],
      "metadata": {
        "id": "skFa-bKcfFkr"
      }
    }
  ]
}